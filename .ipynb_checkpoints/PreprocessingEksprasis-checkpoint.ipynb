{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERxLjQhWGaR2"
   },
   "source": [
    "<img src=\"https://2.bp.blogspot.com/-066qpJs0Ttc/WiYPXGNYEYI/AAAAAAAAFu8/XbOaf7DqfDMM9truu3DkrkIGfRgP4zBzgCLcBGAs/s1600/udinus.jpg\"  width=\"200\">\n",
    "\n",
    "# AMS - ekphrasis Reguler Expression\n",
    "\n",
    "oleh: Dr. Eng. Farrikh Alzami M.Kom; Abu Salam, M.Kom\n",
    "\n",
    "disini kita akan belajar menggunakan pembersihan kata dengan modul `Eksprasis`\n",
    "\n",
    "detail lengkap ada disini: https://github.com/cbaziotis/ekphrasis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wDJWje_GjC1"
   },
   "source": [
    "## install library ekphrasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6142,
     "status": "ok",
     "timestamp": 1665402305421,
     "user": {
      "displayName": "farrikh alzami",
      "userId": "11964535993149439504"
     },
     "user_tz": -420
    },
    "id": "yVnJu5rLGll3",
    "outputId": "09789236-8148-41ac-95b4-d28bf8cbbdf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting ekphrasis\n",
      "  Downloading ekphrasis-0.5.4-py3-none-any.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 1.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.21.6)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 1.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (2.0.1)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.7)\n",
      "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (5.5.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.64.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.4.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.11.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->ekphrasis) (4.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->ekphrasis) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (7.1.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (2022.6.2)\n",
      "Installing collected packages: ftfy, colorama, ekphrasis\n",
      "Successfully installed colorama-0.4.5 ekphrasis-0.5.4 ftfy-6.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ekphrasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35535,
     "status": "ok",
     "timestamp": 1665402353083,
     "user": {
      "displayName": "farrikh alzami",
      "userId": "11964535993149439504"
     },
     "user_tz": -420
    },
    "id": "-K8JCsz3GTev",
    "outputId": "60871e46-23d0-4ca2-f20f-74a9b0dc6b76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word statistics files not found!\n",
      "Downloading... done!\n",
      "Unpacking... done!\n",
      "Reading twitter - 1grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
      "Reading twitter - 2grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    #annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",'emphasis', 'censored'},\n",
    "    annotate={\"hashtag\",\"allcaps\",\"elongated\",\"repeated\",'emphasis','censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1665402353083,
     "user": {
      "displayName": "farrikh alzami",
      "userId": "11964535993149439504"
     },
     "user_tz": -420
    },
    "id": "-RsRWIE1GZhF"
   },
   "outputs": [],
   "source": [
    "def bersih_eks(kata):\n",
    "  return \" \".join(text_processor.pre_process_doc(kata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1665402455059,
     "user": {
      "displayName": "farrikh alzami",
      "userId": "11964535993149439504"
     },
     "user_tz": -420
    },
    "id": "43JbCIPcHOrZ"
   },
   "outputs": [],
   "source": [
    "kalimat = \"@alzami1986 berkata bahwa email alzamiCakep@gmail.com bukan emailnya, 100% yakin bukan emailnya, nomor 08564111111 juga bukan nomor hp nya maka jika ada yang meminta uang $100 atau Rp. 1.000.000 yang harus dikirim tanggal 06-10-2022 pukul 10:30 harap jangan diikuti. silakan hubungi 110. karena mereka adalah PENIPUUUUUU yang suka makan-makan 'uang tidak halal' !!! dan termasuk dalam golongan >:) #scam #penipu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1665402455528,
     "user": {
      "displayName": "farrikh alzami",
      "userId": "11964535993149439504"
     },
     "user_tz": -420
    },
    "id": "79ci5nPWISGs"
   },
   "outputs": [],
   "source": [
    "hasil01 = bersih_eks(kalimat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1665402455529,
     "user": {
      "displayName": "farrikh alzami",
      "userId": "11964535993149439504"
     },
     "user_tz": -420
    },
    "id": "DqafR0JaIWNt",
    "outputId": "485d005e-df82-4c4a-9160-5ad508e073e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<user> berkata bahwa email <email> bukan emailnya , <percent> yakin bukan emailnya , nomor <number> juga bukan nomor hp nya maka jika ada yang meminta uang <money> atau rp . <number> . <number> yang harus dikirim tanggal <date> pukul <time> harap jangan diikuti . silakan hubungi <number> . karena mereka adalah <allcaps> penipu <elongated> </allcaps> yang suka makan - makan ' uang tidak halal ' ! <repeated> dan termasuk dalam golongan <devil> <hashtag> scam </hashtag> <hashtag> penipu </hashtag>\n"
     ]
    }
   ],
   "source": [
    "print(hasil01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1665402521263,
     "user": {
      "displayName": "farrikh alzami",
      "userId": "11964535993149439504"
     },
     "user_tz": -420
    },
    "id": "RhEucQfNIXQx",
    "outputId": "371fa6d9-a226-4dce-dc99-275356e0f2f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penipu <elongated>\n"
     ]
    }
   ],
   "source": [
    "kalimat2 = 'peeeeniiiiippuuuuuu'\n",
    "hasil02 = bersih_eks(kalimat2)\n",
    "print(hasil02)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMqoYYzrTifnu1RGmIEio7i",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
